# -*- coding: utf-8 -*-
"""TimeSeries.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vq7Qs9Cph3OcblJ9dFpF8mFWaei3s8-m
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pyspark

!pip install findspark

import json
import pandas as pd
import pyspark
import findspark 
findspark.init()
import pyspark
from pyspark.sql import SparkSession
from pyspark import SparkContext as sc
from pyspark.sql import functions as F
from pyspark.sql.window import Window
import seaborn as sns
import datetime

spark = SparkSession.builder.master("local[1]").appName("bigData").getOrCreate()

#Dataframe
df = spark.read.json('/content/drive/MyDrive/youstatanalyzer10k.json')\
.select(F.col("_id").alias("id"), "day", "views").limit(2000)

df.show()
df.count()

dataDF = df.rdd.map(lambda x : (x[0], x[1][0], x[2][0][0]))
#print(dataDF.collect())

cols = ["id", "date", "views"]
renamedDf = dataDF.toDF(cols).withColumn("dateAndViews", F.arrays_zip("date", "views"))
dateDf = renamedDf.select("id", F.explode("dateAndViews"))
viewsDf = dateDf.withColumn("date", F.expr("from_unixtime(col.date/1000, 'yyyy-MM-dd')")).orderBy("id", "date")\
.select("id", "date", "col.views")

# Getting max views per video
viewsDf.groupBy("id").agg(F.max("views").alias("max_views")).orderBy(F.desc("max_views")).show()

# Graph plotting for the video_ids
import plotly.express as px

fig = px.line(viewsDf.toPandas(), x='date', y='views', color='id')
fig.show()

#plotting of video ID with spike
# used to generate graphs for video ids associated with low and high views 

import plotly.express as px

fig = px.line(viewsDf.filter(F.col("id")=="-2U0Ivkn2Ds").toPandas(), x='date', y='views', color='id')
fig.show()

#Mean, Median, StdDev
meanMedDf = viewsDf.groupBy("id").agg(F.min("views").alias("minViews"), \
                                    F.max("views").alias("maxViews"), \
                                    F.round(F.avg("views")).cast('int').alias("Mean"),
                                    F.round(F.stddev("views")).cast('int').alias("StdDev"))

groupDf = meanMedDf.withColumn('median', ((F.col("minViews")+F.col("maxViews"))/2).cast('int')).orderBy(F.desc(("StdDev")))
#.withColumn('Deviation', F.abs(col("median")-col("mean"))).orderBy(F.desc("Deviation"))

groupDf.show()

# identifying video ids with low views using increasing order of StdDev
groupDf_asc = meanMedDf.withColumn('median', ((F.col("minViews")+F.col("maxViews"))/2).cast('int')).orderBy(F.asc_nulls_last(("StdDev")))
groupDf_asc.show()

groupDf.select(F.max("StdDev"), F.min("Stddev") ).show()

# Categorizing video ids as low and high based on standard deviation threshold ranges
resDf = groupDf.withColumn("threshold", F.when(groupDf.StdDev < 50000, F.lit("videoLow")).otherwise(F.lit("videoHigh")))
resDf.show(300)

# Summary count of video ids
summaryDf = resDf.groupBy("threshold").count().alias("VideoCount")
summaryDf.show()